# -*- coding: utf-8 -*-
"""Visualslide v2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o9aKpYI2ohxaVjiXyGSbobnnOA4uUspx
"""

pip install tensorflow numpy matplotlib pillow

pip install torch transformers datasets tqdm Pillow scikit-learn ipython

pip install datasets pandas

pip install chromadb

pip install opencv-python

from datasets import load_dataset
import pandas as pd
import chromadb # Added import

reference_images=load_dataset("DHPR/Driving-Hazard-Prediction-and-Reasoning")

reference_images=reference_images['train'].to_pandas()

client = chromadb.Client()
reference_collection = client.get_or_create_collection("my_collection")

for i in range(2000):
  reference_collection.add(ids=str(i), documents=reference_images['hazard'].iloc[i])

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import re
import json

model_name = "Qwen/Qwen3-0.6b"

# Load model and tokenizer once
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else "auto",
    device_map="auto"
)

def frame_identifier(question, video_description):
    prompt = f"""
You are a smart Agent that looks at a video with the following description: "{video_description}"
to answer the query: "{question}".

You have a tool that can give you any frame you describe.
Which frames would you want to see to answer the question?

Return your response **strictly** in the following list format, describing at least 5 frames you would want to see:

Example of correct format:
[
  "Frame showing me waiting in the red light",
  "Frame showing me turn right"
]
"""

    messages = [{"role": "user", "content": prompt}]
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)

    generated_ids = model.generate(**model_inputs, max_new_tokens=512)
    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
    content = tokenizer.decode(output_ids, skip_special_tokens=True).strip()

    # --- CLEAN EXTRACTION LOGIC ---
    frames_to_search = []

    try:
        # Find the first Python-style list in the output
        list_match = re.search(r"\[.*?\]", content, re.DOTALL)
        if list_match:
            raw_list = list_match.group(0)
            # Normalize quotes for JSON parsing
            raw_list = raw_list.replace("'", '"')
            frames_to_search = json.loads(raw_list)

            # Ensure it's a list of strings only
            if not all(isinstance(item, str) for item in frames_to_search):
                frames_to_search = [str(item) for item in frames_to_search]
        else:
            print("No list found in model output.")
            print("Raw output:", content)
    except Exception as e:
        print("Parsing error:", e)
        print("Raw output:", content)

    # Always return a list of strings
    return frames_to_search if isinstance(frames_to_search, list) else []

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import re
import json

model_name = "Qwen/Qwen3-0.6b"

# Load model and tokenizer once
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else "auto",
    device_map="auto"
)

def concluder(question, insights, model, tokenizer):
    """
    Summarizes insights with respect to a question using a language model.

    Args:
        question (str): The main question.
        insights (str): The insights gathered.
        model (transformers.PreTrainedModel): The pre-trained language model.
        tokenizer (transformers.PreTrainedTokenizer): The tokenizer for the model.

    Returns:
        str: The generated conclusion.
    """
    prompt = f"""
You are a smart Agent that who has been trying to answer the question {question}.
While working on the question, you have had some insights {insights}.
Summarize the insights with respect to the question for the reader ".
"""

    messages = [{"role": "user", "content": prompt}]

    # Use the `apply_chat_template` method with a `disable_thinking` parameter.
    # The `enable_thinking=False` parameter, if supported by the model's tokenizer,
    # will disable the "thinking" process.
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)
    model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)

    # The `generate` method will handle the generation of the response.
    # The `skip_special_tokens` parameter should be used when decoding the generated tokens.
    generated_ids = model.generate(**model_inputs, max_new_tokens=512)
    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
    content = tokenizer.decode(output_ids, skip_special_tokens=True).strip()

    return content

import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras import layers, models
import numpy as np
from PIL import Image
import io

# ✅ Build MobileNetV2 + Dense(384) projection
base_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg')
output = layers.Dense(384, activation=None, name='embedding_layer')(base_model.output)
feature_model = models.Model(inputs=base_model.input, outputs=output)

def extract_features(img_input):
    """Loads and preprocesses an image (from path, PIL Image, or dictionary), then extracts 384-D features."""
    if isinstance(img_input, str):
        img = Image.open(img_input).convert('RGB')
    elif isinstance(img_input, Image.Image):
        img = img_input.convert('RGB')
    elif isinstance(img_input, dict) and 'bytes' in img_input:
        img = Image.open(io.BytesIO(img_input['bytes'])).convert('RGB')
    else:
        raise TypeError("Input must be a file path, PIL Image, or dict with 'bytes' key")

    # Resize to model input size
    img = img.resize((224, 224))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)

    # Extract features
    features = feature_model.predict(img_array)

    # Flatten to 1D NumPy array
    return features.flatten()

# Example usage:
vec = extract_features(reference_images['image'].iloc[8])
print(vec.shape)   # (384,)

from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image

visual_model = AutoModelForCausalLM.from_pretrained(
    "vikhyatk/moondream2",
    revision="2025-06-21",
    trust_remote_code=True,
    device_map="auto"  # Changed to "auto"
)

def image_reasoning(img: Image.Image, question: str) -> str:
  return(visual_model.query(img, question)["answer"])

import chromadb
import cv2
import numpy as np
from PIL import Image
from io import BytesIO
import matplotlib.pyplot as plt

def extract_frames(video_path, question: str, video_description:str, threshold=30.0):

    list_queries = frame_identifier(question, video_description)

    frames = []
    cap = cv2.VideoCapture(video_path)

    success, prev_frame = cap.read()
    if not success:
        return frames

    # Convert first frame to RGB + PIL
    prev_rgb = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2RGB)
    prev_pil = Image.fromarray(prev_rgb)
    frames.append(prev_pil)

    while True:
        success, frame = cap.read()
        if not success:
            break

        # Compare with previous frame
        diff = cv2.absdiff(frame, prev_frame)
        gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
        non_zero_count = np.count_nonzero(gray)

        # Ratio of changed pixels
        change_ratio = (non_zero_count / gray.size) * 100

        if change_ratio > threshold:
            # Significant change → treat as keyframe
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(frame_rgb)
            frames.append(pil_img)
            prev_frame = frame  # Update reference frame

    cap.release()
    answer = "Analyzing the video, I have found that:"

    client = chromadb.Client()
    # Assuming "my_collection" for images exists and is populated with embeddings from cell 7bUo1LyJ-OM2
    # If not, this will create an empty collection, which might not be intended.
    picture_collection = client.get_or_create_collection("my_collection")

    for i in range(0,len(frames),50):
      vec=extract_features(frames[i])
      picture_collection.add(ids=str(i), embeddings=[vec])

    for i, reference_query in enumerate(list_queries):
        reference_result = reference_collection.query(
            query_texts=[reference_query],
            n_results=1
        )
        reference_result = reference_collection.query(query_texts=reference_query,n_results=1)
        ids = int(reference_result['ids'][0][0])

      # Assuming reference_images is available from cell 7bUo1LyJ-OM2
        query_vec = extract_features(reference_images['image'].iloc[int(ids)]) # Use int(index)

        frame_results = picture_collection.query(query_embeddings=[query_vec], n_results=1)
        index = int(frame_results['ids'][0][0])

        s=image_reasoning(frames[int(index)],"Describe the image and what may be some problems in image") # Use int(id) and access the question
        answer=answer+" "+s

    return concluder(question=question, insights = answer, model=model, tokenizer=tokenizer)

import gradio as gr

logo = 'hunyuanimage-30_in-ghibli-way-draw-a-rabbit-that.png'

custom_css = """
    .logo-img {
        width: 150px !important;  /* Use !important to override defaults */
        height: auto;             /* Maintain aspect ratio */
        margin: 10px auto;        /* Center the logo and add vertical margin */
        display: block;           /* Center the image */
    }
"""

with gr.Blocks(css=custom_css) as demo:
    gr.Image(
        logo,
        interactive=False,
        show_label=False,
        show_download_button=False,
        elem_classes="logo-img"  # Apply the CSS class here
    )

    gr.Markdown("Carrotview")


    with gr.Row():
        video_input = gr.Video(label="Upload a video")
        with gr.Column():
            question_input = gr.Textbox(label="Enter your question")
            video_description_input = gr.Textbox(label="Provide additional context")

    with gr.Row():
        output_text = gr.Textbox(
          label="Answer",
          lines=10,        # number of visible lines (height)
          max_lines=20,    # optional upper bound for auto expansion
          show_copy_button=True  # optional, makes it easy to copy output
        )


    submit_button = gr.Button("Submit")
    submit_button.click(
        fn=extract_frames,
        inputs=[video_input, question_input, video_description_input],
        outputs=output_text
    )

if __name__ == "__main__":
    demo.launch(debug=True, allowed_paths=["./assets"])
